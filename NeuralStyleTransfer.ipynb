{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Style Transfer\n",
    "\n",
    "This is an implementation of Neural Style Transfer first described in 2015 by Gatys et. al <sup>[[1]](#References)</sup>. The mathematical descriptions are informed by Andrew Ng on YouTube<sup>[[2, 3]](#References)</sup> and the implementation details have borrowed heavily from the offical Keras<sup>[[4]](#References)</sup> NST example. I also get other ideas from Tensorflow <sup>[[5, 6]](#References)</sup> regarding total variation loss and an alternative optimisation method. The extention of this is to perform NST in real-time as described in 2016 by Justin Johnson<sup>[[7]](#References)</sup>.\n",
    "\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.preprocessing.image import load_img, save_img, img_to_array\n",
    "import numpy as np\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "import time\n",
    "\n",
    "from keras.applications import vgg16\n",
    "from keras import backend as K\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_image_path = \"./content_image_1.jpg\"\n",
    "style_image_path = \"./style_image_1.jpg\"\n",
    "iterations = 20\n",
    "\n",
    "# these are the weights of the different loss components\n",
    "total_variation_weight = 1.0 # 1.0\n",
    "style_weight = 1.0 # 1.0\n",
    "content_weight = 0.025 # 0.025\n",
    "\n",
    "# dimensions of the generated picture.\n",
    "img_nrows = 224\n",
    "img_ncols = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# util function to open, resize and format pictures into appropriate tensors\n",
    "def preprocess_image(image_path):\n",
    "    img = load_img(image_path, target_size=(img_nrows, img_ncols))\n",
    "    img = img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = vgg16.preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "# util function to convert a tensor into a valid image\n",
    "def deprocess_image(x):\n",
    "    x = x.reshape((img_nrows, img_ncols, 3))\n",
    "    # Remove zero-center by mean pixel\n",
    "    x[:, :, 0] += 103.939\n",
    "    x[:, :, 1] += 116.779\n",
    "    x[:, :, 2] += 123.68\n",
    "    # 'BGR'->'RGB'\n",
    "    x = x[:, :, ::-1]\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve computational efficiency we can store the generated, content and style image as tensors, and allocate them as input to the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/simon/miniconda3/envs/DL/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# get tensor representations of our images\n",
    "content_image = K.variable(preprocess_image(content_image_path), name=\"Content\")\n",
    "style_image = K.variable(preprocess_image(style_image_path), name=\"Style\")\n",
    "\n",
    "# this will contain our generated image\n",
    "generated_image = K.placeholder((1, img_nrows, img_ncols, 3), name=\"Generated\")\n",
    "\n",
    "# combine the 3 images into a single Keras tensor\n",
    "input_tensor = K.concatenate([content_image,\n",
    "                              style_image,\n",
    "                              generated_image], axis=0)\n",
    "\n",
    "# Create index dict\n",
    "index = {\"C\" : 0, \"S\" : 1, \"G\" : 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "# build the VGG16 network with our 3 images as input\n",
    "# the model will be loaded with pre-trained ImageNet weights\n",
    "model = vgg16.VGG16(input_tensor=input_tensor,\n",
    "                    weights='imagenet', include_top=False)\n",
    "print('Model loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
    "outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_1': <tf.Tensor 'concat:0' shape=(3, 224, 224, 3) dtype=float32>,\n",
       " 'block1_conv1': <tf.Tensor 'block1_conv1/Relu:0' shape=(3, 224, 224, 64) dtype=float32>,\n",
       " 'block1_conv2': <tf.Tensor 'block1_conv2/Relu:0' shape=(3, 224, 224, 64) dtype=float32>,\n",
       " 'block1_pool': <tf.Tensor 'block1_pool/MaxPool:0' shape=(3, 112, 112, 64) dtype=float32>,\n",
       " 'block2_conv1': <tf.Tensor 'block2_conv1/Relu:0' shape=(3, 112, 112, 128) dtype=float32>,\n",
       " 'block2_conv2': <tf.Tensor 'block2_conv2/Relu:0' shape=(3, 112, 112, 128) dtype=float32>,\n",
       " 'block2_pool': <tf.Tensor 'block2_pool/MaxPool:0' shape=(3, 56, 56, 128) dtype=float32>,\n",
       " 'block3_conv1': <tf.Tensor 'block3_conv1/Relu:0' shape=(3, 56, 56, 256) dtype=float32>,\n",
       " 'block3_conv2': <tf.Tensor 'block3_conv2/Relu:0' shape=(3, 56, 56, 256) dtype=float32>,\n",
       " 'block3_conv3': <tf.Tensor 'block3_conv3/Relu:0' shape=(3, 56, 56, 256) dtype=float32>,\n",
       " 'block3_pool': <tf.Tensor 'block3_pool/MaxPool:0' shape=(3, 28, 28, 256) dtype=float32>,\n",
       " 'block4_conv1': <tf.Tensor 'block4_conv1/Relu:0' shape=(3, 28, 28, 512) dtype=float32>,\n",
       " 'block4_conv2': <tf.Tensor 'block4_conv2/Relu:0' shape=(3, 28, 28, 512) dtype=float32>,\n",
       " 'block4_conv3': <tf.Tensor 'block4_conv3/Relu:0' shape=(3, 28, 28, 512) dtype=float32>,\n",
       " 'block4_pool': <tf.Tensor 'block4_pool/MaxPool:0' shape=(3, 14, 14, 512) dtype=float32>,\n",
       " 'block5_conv1': <tf.Tensor 'block5_conv1/Relu:0' shape=(3, 14, 14, 512) dtype=float32>,\n",
       " 'block5_conv2': <tf.Tensor 'block5_conv2/Relu:0' shape=(3, 14, 14, 512) dtype=float32>,\n",
       " 'block5_conv3': <tf.Tensor 'block5_conv3/Relu:0' shape=(3, 14, 14, 512) dtype=float32>,\n",
       " 'block5_pool': <tf.Tensor 'block5_pool/MaxPool:0' shape=(3, 7, 7, 512) dtype=float32>}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the Content Loss\n",
    "\n",
    "The content loss <sup>[[2](#References)]</sup> is designed to maintain the \"content\" of the **Content Image** $(C)$ in the **Generated Image** $(G)$. Using the pre-trained network, we can get the activations of both images from difference layers i.e. \n",
    "\n",
    "$$\n",
    "a^{[l](C)} \\text{ and } a^{[l](G)}\n",
    "$$\n",
    "\n",
    "If these activations are similar, both images have similar content. To compare them we simply take the sum of the squared element-wise difference between the activations of our layers of interest, and optionally use a normalisation constant.\n",
    "\n",
    "$$\n",
    "J_{content}(C, G) = \\dfrac{1}{2} {|| a^{[l](C)} - a^{[l](G)} ||}^2\n",
    "$$\n",
    "\n",
    "Because the content of an image tends to be large abstract structures, layers tend to be only the last layer of the network, rather than all layers. However, you could explore this to find out the effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_loss(content, generated):\n",
    "    return K.sum(K.square(generated - content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the Style Loss\n",
    "\n",
    "To compute the style loss<sup>[[3](#References)]</sup>, we must first develop a way to describe style. This is done with the gram matrix. The gram matrix of an image tensor (feature-wise outer product) is a way of measuring the correlation between filers in a layer. We are asking, if filter 1 is firing, is filter 2 firing? We want to know the correlation between all filters, and so the gram matrix is a (feature $\\times$ feature) matrix. We do this my summing the product of the activations in each channel, and multiplying them together. We ant to calculate the *style* of the **Style Image** $S$ and the **Geneterated Image** $(G)$.\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{Let } a_{i, j k}^{[l]} = \\text{ activation at } (i, j, k). G^{[l](S)} \\text{ is } n_c^{[l]} \\times n_c^{[l]}, \\text{ where } i = h, j = w, k = c\n",
    "$$\n",
    "\n",
    "$$\n",
    "G_{kk'}^{[l](S)} = \\sum_{i=1}^{n_h^{[l]}} \\sum_{j=1}^{n_w^{[l]}} a_{i, j, k}^{[l]} \\cdot a_{i, j, k'}^{[l]}\n",
    "$$\n",
    "\n",
    "$$\n",
    "G_{kk'}^{[l](G)} = \\sum_{i=1}^{n_h^{[l]}} \\sum_{j=1}^{n_w^{[l]}} a_{i, j, k}^{[l]} \\cdot a_{i, j, k'}^{[l]}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "G_{kk'}^{[l]}, \\text{ where } k = 1, ..., n_{c}^{[l]}\n",
    "$$\n",
    "\n",
    "From this we can see that the gram matrix can be computed using the dot product of the channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(x):\n",
    "    assert K.ndim(x) == 3\n",
    "    features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))\n",
    "    print(\"Gram matrix features:\", features.shape)\n",
    "    gram = K.dot(features, K.transpose(features))\n",
    "    return gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to create a loss that captures the difference in style, and of course want want to minimise it. Therefore, the **style loss** is designed to maintain the style of the reference image in the generated image. It is based on the gram matrices (which capture style) of feature maps from the style reference image and from the generated image. We are wanting to minimise the (scaled) sum of squared errors (differences).\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "J_{style}^{[l]}(S, G) &= \\lambda {|| G^{[l](S)} - G^{[l](G)} ||}^2 \\\\\n",
    "                      &= \\lambda \\sum_{k}\\sum_{k'} (G_{kk'}^{[l](S)} - G_{kk'}^{[l](G)})^2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\lambda = \\dfrac{1}{(2 \\cdot n_h^{[l]} n_w^{[l]}n_c^{[l]})^2}\n",
    "$$\n",
    "\n",
    "Notice the normalisation constant term at the beginning. Andrew Ng says it doesn't matter that much because we are scaling the loss with a hyperparamter $\\beta$ later, but we will include it anyway.\n",
    "\n",
    "The overall style loss is computed as follows:\n",
    "\n",
    "$$\n",
    "J_{style}(S, G) = \\sum_l \\lambda^{l} J_{style}^{[l]}(S, G)\n",
    "$$\n",
    "\n",
    "This allows us to weight the correlations between different layers differently to compute the overall style loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_loss(style, generated):\n",
    "    assert K.ndim(style) == 3\n",
    "    assert K.ndim(generated) == 3\n",
    "    S = gram_matrix(style)\n",
    "    C = gram_matrix(generated)\n",
    "    channels = 3\n",
    "    return K.sum(K.square(S - C)) / (4.0 * (channels ** 2) * (img_nrows**2) * (img_ncols ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the Total Variation Loss\n",
    "\n",
    "The **Total Variation Loss** is an additional loss term included in [4](#References) and detailed in [5](#References). It is designed to keep the generated image locally coherent. In other words, it can be used to suppress noise in images. According to Tensorflow, the total varriation is the sum of the **absolute differences** for neighboring pixel-values in the input images. It measures how much noise is in the images and is defined as:\n",
    "\n",
    "$$\n",
    "J_{\\text{total variation}}(G)\n",
    "$$\n",
    "\n",
    "In the case of the Keras code, I have no idea what it is doing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_variation_loss(x):\n",
    "    a = K.square(\n",
    "            x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, 1:, :img_ncols - 1, :])\n",
    "    b = K.square(\n",
    "            x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, :img_nrows - 1, 1:, :])\n",
    "    return K.sum(K.pow(a + b, 1.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the Overall Cost\n",
    "\n",
    "Finally, we can then combine the three losses to get:\n",
    "\n",
    "$$\n",
    "J(G) = \\alpha \\cdot J_{\\text{content}}(C, G) + \\beta \\cdot J_{\\text{style}}(S, G) + \\gamma \\cdot J_{\\text{total variation}}(G)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Variable += will be deprecated. Use variable.assign_add if you want assignment to the variable value or 'x = x + y' if you want a new python Tensor object.\n",
      "Gram matrix features: (64, 50176)\n",
      "Gram matrix features: (64, 50176)\n",
      "Gram matrix features: (128, 12544)\n",
      "Gram matrix features: (128, 12544)\n",
      "Gram matrix features: (256, 3136)\n",
      "Gram matrix features: (256, 3136)\n",
      "Gram matrix features: (512, 784)\n",
      "Gram matrix features: (512, 784)\n",
      "Gram matrix features: (512, 196)\n",
      "Gram matrix features: (512, 196)\n"
     ]
    }
   ],
   "source": [
    "loss = K.variable(0.0)\n",
    "\n",
    "# Select layer features for Content Loss\n",
    "layer_features = outputs_dict['block5_conv2']\n",
    "# As batch process, we can grab the specific features in batch\n",
    "content_features = layer_features[index[\"C\"], :, :, :]\n",
    "generated_features = layer_features[index[\"G\"], :, :, :]\n",
    "\n",
    "# 1. Add the content loss\n",
    "loss += content_weight * content_loss(content_features, generated_features)\n",
    "# Select layers for Style Loss\n",
    "feature_layers = ['block1_conv1', 'block2_conv1',\n",
    "                  'block3_conv1', 'block4_conv1',\n",
    "                  'block5_conv1']\n",
    "\n",
    "# 2. Add each layer to the total loss\n",
    "for layer_name in feature_layers:\n",
    "    layer_features = outputs_dict[layer_name]\n",
    "    style_features = layer_features[index[\"S\"], :, :, :]\n",
    "    generated_features = layer_features[index[\"G\"], :, :, :]\n",
    "    # Get style loss for layer and weight it\n",
    "    sl = style_loss(style_features, generated_features)\n",
    "    loss += (style_weight / len(feature_layers)) * sl\n",
    "\n",
    "# 3. Add the total variation loss from the generated image    \n",
    "loss += total_variation_weight * total_variation_loss(generated_image)\n",
    "\n",
    "# IMPORTANT! Note the deprication warning for \"+=\". If you use variable.assign_add the K.gradients functions fails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To optimize the generated image $(G)$ we will change the values via gradient descent as follows:\n",
    "\n",
    "$$\n",
    "G := G - \\dfrac{\\partial}{\\partial G} J(G)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the gradients of the generated image wrt the loss\n",
    "grads = K.gradients(loss, generated_image)\n",
    "\n",
    "# Create a function to retrieve both the loss (first) and gradients (second)\n",
    "outputs = [loss]\n",
    "if isinstance(grads, (list, tuple)):\n",
    "    outputs += grads\n",
    "else:\n",
    "    outputs.append(grads)\n",
    "\n",
    "# Function\n",
    "f_outputs = K.function([generated_image], outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make things easier to deal with we will create a wrapper function and wrapper class for the optimisation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loss_and_grads(x):\n",
    "    x = x.reshape((1, img_nrows, img_ncols, 3))\n",
    "    outs = f_outputs([x])\n",
    "    loss_value = outs[0]\n",
    "    if len(outs[1:]) == 1:\n",
    "        grad_values = outs[1].flatten().astype('float64')\n",
    "    else:\n",
    "        grad_values = np.array(outs[1:]).flatten().astype('float64')\n",
    "    return loss_value, grad_values\n",
    "\n",
    "class Evaluator(object):\n",
    "    \"\"\"\n",
    "    This Evaluator class makes it possible to compute loss and gradients in one pass\n",
    "    while retrieving them via two separate functions, \"loss\" and \"grads\". This is done\n",
    "    because scipy.optimize requires separate functions for loss and gradients,\n",
    "    but computing them separately would be inefficient.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.loss_value = None\n",
    "        self.grads_values = None\n",
    "\n",
    "    def loss(self, x):\n",
    "        assert self.loss_value is None\n",
    "        loss_value, grad_values = eval_loss_and_grads(x)\n",
    "        self.loss_value = loss_value\n",
    "        self.grad_values = grad_values\n",
    "        return self.loss_value\n",
    "\n",
    "    def grads(self, x):\n",
    "        assert self.loss_value is not None\n",
    "        grad_values = np.copy(self.grad_values)\n",
    "        self.loss_value = None\n",
    "        self.grad_values = None\n",
    "        return grad_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to test it out. Following the directions of [[4]](#References), the following example will use the scipy-based optimisation method - [L-BFGS](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_l_bfgs_b.html). We will later do the same thing using tensorflow and the Adam optimiser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of iteration 0\n",
      "Current loss value: 2826180400.0\n",
      "Image saved as ./generated/000_generated.png\n",
      "Iteration 0 completed in 6s\n",
      "Start of iteration 1\n",
      "Current loss value: 1480848900.0\n",
      "Image saved as ./generated/001_generated.png\n",
      "Iteration 1 completed in 2s\n",
      "Start of iteration 2\n",
      "Current loss value: 1174970500.0\n",
      "Image saved as ./generated/002_generated.png\n",
      "Iteration 2 completed in 2s\n",
      "Start of iteration 3\n",
      "Current loss value: 1048482300.0\n",
      "Image saved as ./generated/003_generated.png\n",
      "Iteration 3 completed in 3s\n",
      "Start of iteration 4\n",
      "Current loss value: 975706100.0\n",
      "Image saved as ./generated/004_generated.png\n",
      "Iteration 4 completed in 3s\n",
      "Start of iteration 5\n",
      "Current loss value: 933364500.0\n",
      "Image saved as ./generated/005_generated.png\n",
      "Iteration 5 completed in 3s\n",
      "Start of iteration 6\n",
      "Current loss value: 907924740.0\n",
      "Image saved as ./generated/006_generated.png\n",
      "Iteration 6 completed in 3s\n",
      "Start of iteration 7\n",
      "Current loss value: 890081900.0\n",
      "Image saved as ./generated/007_generated.png\n",
      "Iteration 7 completed in 3s\n",
      "Start of iteration 8\n",
      "Current loss value: 876806500.0\n",
      "Image saved as ./generated/008_generated.png\n",
      "Iteration 8 completed in 3s\n",
      "Start of iteration 9\n",
      "Current loss value: 864615000.0\n",
      "Image saved as ./generated/009_generated.png\n",
      "Iteration 9 completed in 3s\n",
      "Start of iteration 10\n",
      "Current loss value: 855429600.0\n",
      "Image saved as ./generated/010_generated.png\n",
      "Iteration 10 completed in 3s\n",
      "Start of iteration 11\n",
      "Current loss value: 846721300.0\n",
      "Image saved as ./generated/011_generated.png\n",
      "Iteration 11 completed in 3s\n",
      "Start of iteration 12\n",
      "Current loss value: 840068200.0\n",
      "Image saved as ./generated/012_generated.png\n",
      "Iteration 12 completed in 3s\n",
      "Start of iteration 13\n",
      "Current loss value: 834956400.0\n",
      "Image saved as ./generated/013_generated.png\n",
      "Iteration 13 completed in 3s\n",
      "Start of iteration 14\n",
      "Current loss value: 830904700.0\n",
      "Image saved as ./generated/014_generated.png\n",
      "Iteration 14 completed in 3s\n",
      "Start of iteration 15\n",
      "Current loss value: 826408300.0\n",
      "Image saved as ./generated/015_generated.png\n",
      "Iteration 15 completed in 3s\n",
      "Start of iteration 16\n",
      "Current loss value: 822117570.0\n",
      "Image saved as ./generated/016_generated.png\n",
      "Iteration 16 completed in 3s\n",
      "Start of iteration 17\n",
      "Current loss value: 818227700.0\n",
      "Image saved as ./generated/017_generated.png\n",
      "Iteration 17 completed in 3s\n",
      "Start of iteration 18\n",
      "Current loss value: 814457700.0\n",
      "Image saved as ./generated/018_generated.png\n",
      "Iteration 18 completed in 3s\n",
      "Start of iteration 19\n",
      "Current loss value: 811311040.0\n",
      "Image saved as ./generated/019_generated.png\n",
      "Iteration 19 completed in 3s\n"
     ]
    }
   ],
   "source": [
    "# Create the evaluator\n",
    "evaluator = Evaluator()\n",
    "\n",
    "# Create an input tensor for the content - setting it as the content image will hasten the process\n",
    "x = preprocess_image(content_image_path)\n",
    "\n",
    "for i in range(iterations):\n",
    "    print('Start of iteration', i)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Optimize the input\n",
    "    x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(), fprime=evaluator.grads, maxfun=20)\n",
    "    print('Current loss value:', min_val)\n",
    "    \n",
    "    # save current generated image\n",
    "    img = deprocess_image(x.copy())\n",
    "    \n",
    "    fname = \"./generated/{:03d}_generated.png\".format(i)\n",
    "    save_img(fname, img)\n",
    "    end_time = time.time()\n",
    "    print('Image saved as', fname)\n",
    "    print('Iteration %d completed in %ds' % (i, end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the final result:\n",
    "\n",
    "\n",
    "<br>\n",
    "<div style=\"text-align: center; font-size: 17px; line-height:20px;\">\n",
    "    <div style=\"display:inline-block; text-align: center;\">\n",
    "        <p style=\"background: yellow; margin:0\">Content Image</p>\n",
    "        <img style=\"margin: 0; padding:0;\" src=\"./content_image_1.jpg\" width=224>\n",
    "    </div>\n",
    "    <div style=\"display:inline-block; text-align: center;\">\n",
    "        <p style=\"background: yellow; margin:0\">Style Image</p>\n",
    "        <img style=\"margin: 0; padding:0;\" src=\"./style_image_1.jpg\" width=224>\n",
    "    </div>\n",
    "    <div style=\"display:inline-block; text-align: center;\">\n",
    "        <p style=\"background: yellow; margin:0\">Generated Image</p>\n",
    "        <img style=\"margin: 0; padding:0;\" src=\"./generated/019_generated.png\" width=224>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "\n",
    "A `gif` of the optimisation process can be created using [ImageMagik](https://www.tecmint.com/install-imagemagick-on-debian-ubuntu/) with help available [here](https://askubuntu.com/questions/648244/how-do-i-create-an-animated-gif-from-still-images-preferably-with-the-command-l).\n",
    "\n",
    "```bash\n",
    "convert -loop 0 -delay 20 ./generated/*.png generated.gif\n",
    "```\n",
    "<div style=\"text-align: center; font-size: 40px;\">\n",
    "    <img src=\"./generated_1.gif\" width=224 style=\"display:inline-block\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insted of using an external library to optimise the image, we can use our own adam optimiser to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam(object):\n",
    "    \n",
    "    \"\"\"Adam optimizer.\n",
    "    Default parameters follow those provided in the original paper.\n",
    "    # Arguments\n",
    "        lr: float >= 0. Learning rate.\n",
    "        beta_1: float, 0 < beta < 1. Generally close to 1.\n",
    "        beta_2: float, 0 < beta < 1. Generally close to 1.\n",
    "        epsilon: float >= 0. Fuzz factor.\n",
    "        decay: float >= 0. Learning rate decay over each update.\n",
    "    # References\n",
    "        - [Adam - A Method for Stochastic Optimization](http://arxiv.org/abs/1412.6980v8)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999,\n",
    "                 epsilon=1e-8, decay=0., **kwargs):\n",
    "        \n",
    "        allowed_kwargs = {'clipnorm', 'clipvalue'}\n",
    "        for k in kwargs:\n",
    "            if k not in allowed_kwargs:\n",
    "                raise TypeError('Unexpected keyword argument '\n",
    "                                'passed to optimizer: ' + str(k))\n",
    "        self.__dict__.update(kwargs)\n",
    "        self.iterations = 0\n",
    "        self.lr = lr\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "        self.decay = decay\n",
    "        self.epsilon = epsilon\n",
    "        self.initial_decay = decay\n",
    "\n",
    "    def get_update(self, params, grads):\n",
    "        \"\"\" params and grads are list of numpy arrays\n",
    "        \"\"\"\n",
    "        original_shapes = [x.shape for x in params]\n",
    "        params = [x.flatten() for x in params]\n",
    "        grads = [x.flatten() for x in grads]\n",
    "        \n",
    "        lr = self.lr\n",
    "        if self.initial_decay > 0:\n",
    "            lr *= (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "        t = self.iterations + 1\n",
    "        lr_t = lr * (np.sqrt(1. - np.power(self.beta_2, t)) /\n",
    "                     (1. - np.power(self.beta_1, t)))\n",
    "\n",
    "        if not hasattr(self, 'ms'):\n",
    "            self.ms = [np.zeros(p.shape) for p in params]\n",
    "            self.vs = [np.zeros(p.shape) for p in params]\n",
    "    \n",
    "        ret = [None] * len(params)\n",
    "        for i, p, g, m, v in zip(range(len(params)), params, grads, self.ms, self.vs):\n",
    "            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
    "            v_t = (self.beta_2 * v) + (1. - self.beta_2) * np.square(g)\n",
    "            p_t = p - lr_t * m_t / (np.sqrt(v_t) + self.epsilon)\n",
    "            self.ms[i] = m_t\n",
    "            self.vs[i] = v_t\n",
    "            ret[i] = p_t\n",
    "        \n",
    "        self.iterations += 1\n",
    "        \n",
    "        for i in range(len(ret)):\n",
    "            ret[i] = ret[i].reshape(original_shapes[i])\n",
    "        \n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case our code is much simplier, however, the optimision process has a different feel, but none the less produces a pleasing result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of iteration 0\n",
      "Current loss value: 98689384000.0\n",
      "Image saved as ./generated_tf/000_generated.png\n",
      "Iteration 0 completed in 0s\n",
      "Start of iteration 1\n",
      "Current loss value: 39688020000.0\n",
      "Image saved as ./generated_tf/001_generated.png\n",
      "Iteration 1 completed in 0s\n",
      "Start of iteration 2\n",
      "Current loss value: 49559130000.0\n",
      "Image saved as ./generated_tf/002_generated.png\n",
      "Iteration 2 completed in 0s\n",
      "Start of iteration 3\n",
      "Current loss value: 16009423000.0\n",
      "Image saved as ./generated_tf/003_generated.png\n",
      "Iteration 3 completed in 0s\n",
      "Start of iteration 4\n",
      "Current loss value: 13658618000.0\n",
      "Image saved as ./generated_tf/004_generated.png\n",
      "Iteration 4 completed in 0s\n",
      "Start of iteration 5\n",
      "Current loss value: 9087184000.0\n",
      "Image saved as ./generated_tf/005_generated.png\n",
      "Iteration 5 completed in 0s\n",
      "Start of iteration 6\n",
      "Current loss value: 13640884000.0\n",
      "Image saved as ./generated_tf/006_generated.png\n",
      "Iteration 6 completed in 0s\n",
      "Start of iteration 7\n",
      "Current loss value: 9029980000.0\n",
      "Image saved as ./generated_tf/007_generated.png\n",
      "Iteration 7 completed in 0s\n",
      "Start of iteration 8\n",
      "Current loss value: 7942221300.0\n",
      "Image saved as ./generated_tf/008_generated.png\n",
      "Iteration 8 completed in 0s\n",
      "Start of iteration 9\n",
      "Current loss value: 7553410000.0\n",
      "Image saved as ./generated_tf/009_generated.png\n",
      "Iteration 9 completed in 0s\n",
      "Start of iteration 10\n",
      "Current loss value: 6527381000.0\n",
      "Image saved as ./generated_tf/010_generated.png\n",
      "Iteration 10 completed in 0s\n",
      "Start of iteration 11\n",
      "Current loss value: 6840822300.0\n",
      "Image saved as ./generated_tf/011_generated.png\n",
      "Iteration 11 completed in 0s\n",
      "Start of iteration 12\n",
      "Current loss value: 6079234000.0\n",
      "Image saved as ./generated_tf/012_generated.png\n",
      "Iteration 12 completed in 0s\n",
      "Start of iteration 13\n",
      "Current loss value: 5101600000.0\n",
      "Image saved as ./generated_tf/013_generated.png\n",
      "Iteration 13 completed in 0s\n",
      "Start of iteration 14\n",
      "Current loss value: 4807615000.0\n",
      "Image saved as ./generated_tf/014_generated.png\n",
      "Iteration 14 completed in 0s\n",
      "Start of iteration 15\n",
      "Current loss value: 4325195000.0\n",
      "Image saved as ./generated_tf/015_generated.png\n",
      "Iteration 15 completed in 0s\n",
      "Start of iteration 16\n",
      "Current loss value: 4119905800.0\n",
      "Image saved as ./generated_tf/016_generated.png\n",
      "Iteration 16 completed in 0s\n",
      "Start of iteration 17\n",
      "Current loss value: 4027291600.0\n",
      "Image saved as ./generated_tf/017_generated.png\n",
      "Iteration 17 completed in 0s\n",
      "Start of iteration 18\n",
      "Current loss value: 3694006800.0\n",
      "Image saved as ./generated_tf/018_generated.png\n",
      "Iteration 18 completed in 0s\n",
      "Start of iteration 19\n",
      "Current loss value: 3527229200.0\n",
      "Image saved as ./generated_tf/019_generated.png\n",
      "Iteration 19 completed in 0s\n",
      "Start of iteration 20\n",
      "Current loss value: 3327088000.0\n",
      "Image saved as ./generated_tf/020_generated.png\n",
      "Iteration 20 completed in 0s\n",
      "Start of iteration 21\n",
      "Current loss value: 3142847000.0\n",
      "Image saved as ./generated_tf/021_generated.png\n",
      "Iteration 21 completed in 0s\n",
      "Start of iteration 22\n",
      "Current loss value: 3073419500.0\n",
      "Image saved as ./generated_tf/022_generated.png\n",
      "Iteration 22 completed in 0s\n",
      "Start of iteration 23\n",
      "Current loss value: 2931469800.0\n",
      "Image saved as ./generated_tf/023_generated.png\n",
      "Iteration 23 completed in 0s\n",
      "Start of iteration 24\n",
      "Current loss value: 2767856000.0\n",
      "Image saved as ./generated_tf/024_generated.png\n",
      "Iteration 24 completed in 0s\n",
      "Start of iteration 25\n",
      "Current loss value: 2675469000.0\n",
      "Image saved as ./generated_tf/025_generated.png\n",
      "Iteration 25 completed in 0s\n",
      "Start of iteration 26\n",
      "Current loss value: 2584150000.0\n",
      "Image saved as ./generated_tf/026_generated.png\n",
      "Iteration 26 completed in 0s\n",
      "Start of iteration 27\n",
      "Current loss value: 2464299000.0\n",
      "Image saved as ./generated_tf/027_generated.png\n",
      "Iteration 27 completed in 0s\n",
      "Start of iteration 28\n",
      "Current loss value: 2385337300.0\n",
      "Image saved as ./generated_tf/028_generated.png\n",
      "Iteration 28 completed in 0s\n",
      "Start of iteration 29\n",
      "Current loss value: 2308295200.0\n",
      "Image saved as ./generated_tf/029_generated.png\n",
      "Iteration 29 completed in 0s\n"
     ]
    }
   ],
   "source": [
    "# Get the input image\n",
    "x =  preprocess_image(\"./jensen.png\")\n",
    "\n",
    "# Create optimiser - Note: very sensitive to learning rate\n",
    "optimiser = Adam(lr=20)\n",
    "\n",
    "# Optimise the input\n",
    "for i in range(30):\n",
    "    print('Start of iteration', i)\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Compute loss and gradients\n",
    "    loss_value, grads = f_outputs([x])\n",
    "    print('Current loss value:', loss_value)\n",
    "    \n",
    "    # Update the Image\n",
    "    x = optimiser.get_update(params=[x],\n",
    "                             grads=[grads.reshape(1, 224, 224, 3)])[0]\n",
    "                   \n",
    "    # Save current generated image\n",
    "    img = deprocess_image(x.copy())\n",
    "\n",
    "    fname = \"./generated_adam/{:03d}_generated.png\".format(i)\n",
    "    save_img(fname, img)\n",
    "    end_time = time.time()\n",
    "    print('Image saved as', fname)\n",
    "    print('Iteration %d completed in %ds' % (i, end_time - start_time))                              \n",
    "                               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the final result:\n",
    "\n",
    "<br>\n",
    "<div style=\"text-align: center; font-size: 17px; line-height:20px;\">\n",
    "    <div style=\"display:inline-block; text-align: center;\">\n",
    "        <p style=\"background: yellow; margin:0\">Content Image</p>\n",
    "        <img style=\"margin: 0; padding:0;\" src=\"./content_image_2.png\" width=224>\n",
    "    </div>\n",
    "    <div style=\"display:inline-block; text-align: center;\">\n",
    "        <p style=\"background: yellow; margin:0\">Style Image</p>\n",
    "        <img style=\"margin: 0; padding:0;\" src=\"./style_image_1.jpg\" width=224>\n",
    "    </div>\n",
    "    <div style=\"display:inline-block; text-align: center;\">\n",
    "        <p style=\"background: yellow; margin:0\">Generated Image</p>\n",
    "        <img style=\"margin: 0; padding:0;\" src=\"./generated_adam/029_generated.png\" width=224>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: center; font-size: 40px;\">\n",
    "    <img src=\"./generated_2.gif\" width=224 style=\"display:inline-block\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to do the whole thing with tensorflow we need to change a couple of the steps. The main thing is setting the generated image as a `variable` rather than a `placeholder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "content_image_path = \"./content_image_3.jpg\"\n",
    "style_image_path = \"./style_image_2.jpg\"\n",
    "\n",
    "# get tensor representations of our images\n",
    "content_image = K.variable(preprocess_image(content_image_path), name=\"Content\")\n",
    "style_image = K.variable(preprocess_image(style_image_path), name=\"Style\")\n",
    "\n",
    "# this will contain our generated image - start with a random image\n",
    "generated_image = K.variable(preprocess_image(content_image_path), name=\"Generated\")\n",
    "\n",
    "# combine the 3 images into a single Keras tensor\n",
    "input_tensor = K.concatenate([content_image,\n",
    "                              style_image,\n",
    "                              generated_image], axis=0)\n",
    "\n",
    "# Create index dict\n",
    "index = {\"C\" : 0, \"S\" : 1, \"G\" : 2}\n",
    "\n",
    "model = vgg16.VGG16(input_tensor=input_tensor,\n",
    "                    weights='imagenet', include_top=False)\n",
    "print('Model loaded.')\n",
    "\n",
    "\n",
    "# get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
    "outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the loss again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Variable += will be deprecated. Use variable.assign_add if you want assignment to the variable value or 'x = x + y' if you want a new python Tensor object.\n",
      "Gram matrix features: (64, 50176)\n",
      "Gram matrix features: (64, 50176)\n",
      "Gram matrix features: (128, 12544)\n",
      "Gram matrix features: (128, 12544)\n",
      "Gram matrix features: (256, 3136)\n",
      "Gram matrix features: (256, 3136)\n",
      "Gram matrix features: (512, 784)\n",
      "Gram matrix features: (512, 784)\n",
      "Gram matrix features: (512, 196)\n",
      "Gram matrix features: (512, 196)\n"
     ]
    }
   ],
   "source": [
    "loss = K.variable(0.0)\n",
    "\n",
    "# Select layer features for Content Loss\n",
    "layer_features = outputs_dict['block5_conv2']\n",
    "# As batch process, we can grab the specific features in batch\n",
    "content_features = layer_features[index[\"C\"], :, :, :]\n",
    "generated_features = layer_features[index[\"G\"], :, :, :]\n",
    "\n",
    "# 1. Add the content loss\n",
    "loss += content_weight * content_loss(content_features, generated_features)\n",
    "# Select layers for Style Loss\n",
    "feature_layers = ['block1_conv1', 'block2_conv1',\n",
    "                  'block3_conv1', 'block4_conv1',\n",
    "                  'block5_conv1']\n",
    "\n",
    "# 2. Add each layer to the total loss\n",
    "for layer_name in feature_layers:\n",
    "    layer_features = outputs_dict[layer_name]\n",
    "    style_features = layer_features[index[\"S\"], :, :, :]\n",
    "    generated_features = layer_features[index[\"G\"], :, :, :]\n",
    "    # Get style loss for layer and weight it\n",
    "    sl = style_loss(style_features, generated_features)\n",
    "    loss += (style_weight / len(feature_layers)) * sl\n",
    "\n",
    "# 3. Add the total variation loss from the generated image    \n",
    "loss += total_variation_weight * total_variation_loss(generated_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to optimise!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of iteration 0\n",
      "Current loss value: 277206760000.0\n",
      "Image saved as ./generated_tf/000_generated.png\n",
      "Iteration 0 completed in 0s\n",
      "Start of iteration 1\n",
      "Current loss value: 203848430000.0\n",
      "Image saved as ./generated_tf/001_generated.png\n",
      "Iteration 1 completed in 0s\n",
      "Start of iteration 2\n",
      "Current loss value: 111929210000.0\n",
      "Image saved as ./generated_tf/002_generated.png\n",
      "Iteration 2 completed in 0s\n",
      "Start of iteration 3\n",
      "Current loss value: 49100190000.0\n",
      "Image saved as ./generated_tf/003_generated.png\n",
      "Iteration 3 completed in 0s\n",
      "Start of iteration 4\n",
      "Current loss value: 61783548000.0\n",
      "Image saved as ./generated_tf/004_generated.png\n",
      "Iteration 4 completed in 0s\n",
      "Start of iteration 5\n",
      "Current loss value: 75000420000.0\n",
      "Image saved as ./generated_tf/005_generated.png\n",
      "Iteration 5 completed in 0s\n",
      "Start of iteration 6\n",
      "Current loss value: 57795236000.0\n",
      "Image saved as ./generated_tf/006_generated.png\n",
      "Iteration 6 completed in 0s\n",
      "Start of iteration 7\n",
      "Current loss value: 42482230000.0\n",
      "Image saved as ./generated_tf/007_generated.png\n",
      "Iteration 7 completed in 0s\n",
      "Start of iteration 8\n",
      "Current loss value: 34538095000.0\n",
      "Image saved as ./generated_tf/008_generated.png\n",
      "Iteration 8 completed in 0s\n",
      "Start of iteration 9\n",
      "Current loss value: 29540667000.0\n",
      "Image saved as ./generated_tf/009_generated.png\n",
      "Iteration 9 completed in 0s\n",
      "Start of iteration 10\n",
      "Current loss value: 25451470000.0\n",
      "Image saved as ./generated_tf/010_generated.png\n",
      "Iteration 10 completed in 0s\n",
      "Start of iteration 11\n",
      "Current loss value: 21941019000.0\n",
      "Image saved as ./generated_tf/011_generated.png\n",
      "Iteration 11 completed in 0s\n",
      "Start of iteration 12\n",
      "Current loss value: 18841397000.0\n",
      "Image saved as ./generated_tf/012_generated.png\n",
      "Iteration 12 completed in 0s\n",
      "Start of iteration 13\n",
      "Current loss value: 16326207000.0\n",
      "Image saved as ./generated_tf/013_generated.png\n",
      "Iteration 13 completed in 0s\n",
      "Start of iteration 14\n",
      "Current loss value: 14983920000.0\n",
      "Image saved as ./generated_tf/014_generated.png\n",
      "Iteration 14 completed in 0s\n",
      "Start of iteration 15\n",
      "Current loss value: 14767141000.0\n",
      "Image saved as ./generated_tf/015_generated.png\n",
      "Iteration 15 completed in 0s\n",
      "Start of iteration 16\n",
      "Current loss value: 14559412000.0\n",
      "Image saved as ./generated_tf/016_generated.png\n",
      "Iteration 16 completed in 0s\n",
      "Start of iteration 17\n",
      "Current loss value: 13501718000.0\n",
      "Image saved as ./generated_tf/017_generated.png\n",
      "Iteration 17 completed in 0s\n",
      "Start of iteration 18\n",
      "Current loss value: 11885608000.0\n",
      "Image saved as ./generated_tf/018_generated.png\n",
      "Iteration 18 completed in 0s\n",
      "Start of iteration 19\n",
      "Current loss value: 10430839000.0\n",
      "Image saved as ./generated_tf/019_generated.png\n",
      "Iteration 19 completed in 0s\n"
     ]
    }
   ],
   "source": [
    "# Create optimizer\n",
    "optimiser = tf.train.AdamOptimizer(learning_rate=10).minimize(loss, var_list=[generated_image])\n",
    "\n",
    "# Using the current Keras session\n",
    "sess = K.get_session()\n",
    "\n",
    "# Optimise the input\n",
    "for i in range(iterations):\n",
    "    print('Start of iteration', i)\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Update compute loss, get gradients & update image\n",
    "    loss_value, _, raw_img = sess.run([loss, optimiser, generated_image])\n",
    "    print('Current loss value:', loss_value)\n",
    "\n",
    "    # Save current generated image\n",
    "    img = deprocess_image(raw_img)\n",
    "    \n",
    "    fname = \"./generated_tf/{:03d}_generated.png\".format(i)\n",
    "    save_img(fname, img)\n",
    "    end_time = time.time()\n",
    "    print('Image saved as', fname)\n",
    "    print('Iteration %d completed in %ds' % (i, end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the final result:\n",
    "\n",
    "\n",
    "<br>\n",
    "<div style=\"text-align: center; font-size: 17px; line-height:20px;\">\n",
    "    <div style=\"display:inline-block; text-align: center;\">\n",
    "        <p style=\"background: yellow; margin:0\">Content Image</p>\n",
    "        <img style=\"margin: 0; padding:0;\" src=\"./content_image_3.jpg\" width=224>\n",
    "    </div>\n",
    "    <div style=\"display:inline-block; text-align: center;\">\n",
    "        <p style=\"background: yellow; margin:0\">Style Image</p>\n",
    "        <img style=\"margin: 0; padding:0;\" src=\"./style_image_2.jpg\" width=224>\n",
    "    </div>\n",
    "    <div style=\"display:inline-block; text-align: center;\">\n",
    "        <p style=\"background: yellow; margin:0\">Generated Image</p>\n",
    "        <img style=\"margin: 0; padding:0;\" src=\"./generated_tf/019_generated.png\" width=224>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: center; font-size: 40px;\">\n",
    "    <img src=\"./generated_3.gif\" width=224 style=\"display:inline-block\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Gatys, L. A., Ecker, A. S., & Bethge, M. (2015). A neural algorithm of artistic style. arXiv preprint [arXiv:1508.06576](https://arxiv.org/abs/1508.06576).\n",
    "2. Content Cost Function - Andrew Ng on [YouTube](https://www.youtube.com/watch?v=b1I5X3UfEYI&list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF&index=40)\n",
    "3. Style Cost Function - Andrew Ng on [YouTube](https://www.youtube.com/watch?v=QgkLfjfGul8&list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF&index=41)\n",
    "4. Neural Style Transfer with [Keras.](https://keras.io/examples/neural_style_transfer/)\n",
    "5. Total Variation Loss with [Tensorflow.](https://www.tensorflow.org/api_docs/python/tf/image/total_variation)\n",
    "6. Neural Style Transfer with [Tensorflow.](https://www.tensorflow.org/beta/tutorials/generative/style_transfer)\n",
    "7. Johnson, J., Alahi, A., & Fei-Fei, L. (2016, October). Perceptual losses for real-time style transfer and super-resolution. [In European conference on computer vision (pp. 694-711). Springer, Cham.](https://cs.stanford.edu/people/jcjohns/papers/eccv16/JohnsonECCV16.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
